---
title: "Predictors of SuperHuman Vision"
author: "Paul Sanfilippo"
date: 2018-10-30T21:13:14-05:00
description: "Under construction..."
categories: ["R"]
tags: ["regression", "plot"]
featured: true
image: "/img/super.png"

---



<style>
div.code pre { 
                font-family: 'Source Code Pro', 'Courier New', monospace;
                font-size: 11px;
                background-color:#F5F8FA;
                padding-top: 10px;
                padding-bottom: 10px;
                padding-left: 10px;
                border: 1px solid lightgrey;
                border-radius: 5px;
}
</style>
<style>
div.output pre { 
                font-family: 'Source Code Pro', 'Courier New', monospace;
                font-size: 11px;
                padding-top: 10px;
                padding-bottom: 10px;
                padding-left: 10px;
                border: 1px solid lightgrey;
                border-radius: 5px;
}
</style>
<p><br></p>
<p>I wanted to model the relationship between various environmental, lifestyle, maternal, childhood and ophthalmic factors and best-corrected <a href="https://en.wikipedia.org/wiki/Visual_acuity">visual acuity</a> (BCVA) in a young adult cohort. Data were available for 1252 patients.</p>
<p>A secondary aim was to compare the predictive power of a classical modelling method (logistic regression) with a machine learning technique (penalised regression). An excellent overview of the differences can be found in this <a href="https://www.nature.com/articles/nmeth.4642">Nature</a> paper.</p>
<p>SuperHuman vision was defined as BCVA better than or equal to 6/4.8 vision (-0.1 logMAR) in at least one eye.<br />
<em>Note</em> - ‘Normal’ vision is considered equivalent to 0.00 logMAR (or 6/6 (20/20) on the Snellen scale more commonly utilised in clinical practice).</p>
<div id="descriptives" class="section level3">
<h3>Descriptives</h3>
<p>First, we’ll load the packages required for this project.</p>
<div class="code">
<pre class="r"><code>library(readxl) # read excel files
library(dplyr) # get data into shape
library(ggplot2) # plots
library(emmeans) # post-hoc model contrasts
library(missForest) # random forest imputation of missing data
library(glmnet) # penalised regression
library(officer) # table output for Word
library(flextable) # table output for Word</code></pre>
</div>
<p>BCVA is currently continuous - we need to dichotomise it into ‘Normal’ - coded as 0, and ‘SuperHuman’ - coded as 1, vision categories.</p>
<div class="code">
<pre class="r"><code>dat$bcva_cat &lt;- ifelse(dat$bcva_r &lt;= -0.1 | dat$bcva_l &lt;= -0.1, 1, 0)</code></pre>
</div>
<p>Let’s do some basic plotting. The frequency histogram for Left and Right eye data, constructed with ggplot2:</p>
<div class="code">
<pre class="r"><code>hist_va &lt;-ggplot(dat, aes(x = bcva, fill = bcva &lt; -0.1)) +
  geom_histogram(color = &quot;black&quot;,  binwidth = 0.05, center = 0.025) +
  scale_x_continuous(limits = c(-0.4, 0.4), breaks = seq(-0.4, 0.4, 0.1)) +
  theme(legend.position = &quot;none&quot;) +
  xlab(&quot;Best Corrected Visual Acuity (logMAR)&quot;) +
  ylab(&quot;Frequency &quot;) +
  scale_fill_manual(values = c(&quot;white&quot;, &quot;red&quot;)) +
  facet_grid(. ~ eye)</code></pre>
</div>
<p>Then overlaying the classification thresholds for SuperHuman vision:</p>
<div class="code">
<pre class="r"><code>hist_va + 
  geom_vline(data = dat, aes(xintercept = -0.097, color = &quot;red&quot;), linetype = &quot;dashed&quot;) +
  geom_text(aes(-0.25, 420, label = &quot;Superhuman Vision&quot;, color = &quot;red&quot;))</code></pre>
</div>
<p>And we end up with:</p>
<p><img src="/figs/svfig1.png" alt="png" width="800"/></p>
<p><br></p>
<p>About <strong>42%</strong> of patients had SuperHuman vision under this definition.</p>
</div>
<div id="classical-modelling-logistic-regression" class="section level3">
<h3>Classical Modelling (Logistic Regression)</h3>
<p>As the dependent (response or outcome) variable in this case is dichotomous (present/absent, yes/no, SuperHuman vision/Normal vision, 1/0), we will use <a href="https://onlinecourses.science.psu.edu/stat504/node/149/">logistic regression</a> to find the best fitting (yet biologically plausible) model to describe the relationship between vision and the set of independent (predictor or explanatory) variables. This model predicts a logit (log-odds) transformation of the probability of a patient having SuperHuman vision.</p>
<p>Potential predictor variables were tested individually and then in a multivariable model, and were initially selected based on biological plausibility of association. The final best-fitting model was:</p>
<div class="code">
<pre class="r"><code>summary(classical_mod &lt;- glm(bcva_cat ~ age + sex + sph + cyl + sum_out + head_circum_age0, 
    data = dat, family = binomial))</code></pre>
</div>
<p>and the output:</p>
<div class="output">
<pre class="r"><code>## Call:
## glm(formula = bcva_cat ~ age + sex + sph + cyl + sum_out + head_circum_age0, 
##     family = binomial, data = dat)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5666  -1.0165  -0.6196   1.1182   2.5297  
## 
## Coefficients:
##                                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                         0.02381    3.53640   0.007  0.99463    
## age                                -0.19074    0.15896  -1.200  0.23018    
## sexM                                0.71379    0.14010   5.095 3.49e-07 ***
## sph                                 0.14556    0.05496   2.649  0.00808 ** 
## cyl                                 1.74377    0.25573   6.819 9.18e-12 ***
## sum_outLess than 1/4 of the day     1.29889    0.57195   2.271  0.02315 *  
## sum_out1/2 of the day               1.36243    0.57260   2.379  0.01734 *  
## sum_outGreater than 3/4 of the day  1.10808    0.59977   1.848  0.06467 .  
## sum_outCannot judge                 0.55005    0.60266   0.913  0.36140    
## head_circum_age0                    0.07685    0.03884   1.979  0.04787 *  
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1377.7  on 1013  degrees of freedom
## Residual deviance: 1247.6  on 1004  degrees of freedom
##   (238 observations deleted due to missingness)
## AIC: 1267.6
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<p>As always, raw output is not very presentable, so let’s try and do some formatting using the flextable package. First, manually assemble the dataframe after generating the 95% confidence intervals (C.I.’s).</p>
<div class="code">
<pre class="r"><code>cc &lt;- coef(summary(classical_mod))
cc &lt;- cc[,-3]
ci &lt;- confint(classical_mod)
citab &lt;- cbind(as.data.frame(cc), as.data.frame(ci))

rownames(citab) &lt;- c(&quot;Intercept&quot;, 
                     &quot;Age&quot;, 
                     &quot;Sex&quot;, 
                     &quot;Refraction - Sphere&quot;, 
                     &quot;Refraction - Cylinder&quot;, 
                     &quot;Time spent outdoors - less than 1/4 day&quot;, 
                     &quot;Time spent outdoors - 1/2 day&quot;, 
                     &quot;Time spent outdoors - more than 3/4 day&quot;, 
                     &quot;Time spent outdoors - cannot judge&quot;, 
                     &quot;Head circumference at birth&quot;)

citab_classical_mod &lt;- cbind(&quot;Variable&quot; = rownames(citab), 
                             citab[,1:2], 
                             &quot;Estimate Lower C.I.&quot; = citab[,4], 
                             &quot;Estimate Upper C.I.&quot; = citab[,5], 
                             &quot;Odds Ratio&quot; = exp(citab[,1]), 
                             &quot;Odds Ratio lower C.I.&quot; = exp(citab[,4]), 
                             &quot;Odds Ratio upper C.I.&quot; = exp(citab[,5]), 
                             &quot;P-value&quot; = citab[,3])</code></pre>
</div>
<p>Then create the table as a Word document.</p>
<div class="code">
<pre class="r"><code>table &lt;- regulartable(citab_classical_mod) %&gt;%
  set_formatter_type(fmt_double = &quot;%0.3f&quot;)
theme_vanilla(table)

doc &lt;- read_docx() %&gt;%
  body_add_flextable(value = table)
print(doc, target = &quot;.../table.docx&quot;) %&gt;% invisible()</code></pre>
</div>
<p>And here are the more nicely formatted results:</p>
<p><img src="/figs/class_table.png" alt="png" width="800"/></p>
<p>So it seems that being male, having low refractive error (spherical and cylindrical), spending more time outdoors, and having a larger head circumference at birth are all associated with higher odds of having SuperHuman vision.</p>
<p>Often, visualisation of the results in the form of a plot is better for understanding and interpretation of effects than simply looking at a table of numbers. Here, we can do this by calculating the estimated marginal means (predicted means) of the log-odds of SuperHuman vision for set combinations of the predictor variables. The response can be converted to a predicted probability of SuperHuman vision which is more helpful in understanding predictor effects.</p>
<p>Let’s do this just for the <code>Time spent outdoors</code> variable.</p>
<p>Calculate the estimated marginal means and convert to predicted probabilities:</p>
<div class="code">
<pre class="r"><code>classical_mod_em &lt;- emmeans(classical_mod, ~ sex + sum_out, type = &quot;response&quot;)</code></pre>
</div>
<div class="output">
<pre class="r"><code>## sex sum_out                          prob         SE  df  asymp.LCL asymp.UCL
##  F   None                        0.1222504 0.06094369 Inf 0.04375343 0.2977293
##  M   None                        0.2214053 0.09738331 Inf 0.08590336 0.4625012
##  F   Less than 1/4 of the day    0.3379598 0.02877234 Inf 0.28405372 0.3964314
##  M   Less than 1/4 of the day    0.5103474 0.03199022 Inf 0.44781308 0.5725595
##  F   1/2 of the day              0.3523200 0.03007300 Inf 0.29584587 0.4132477
##  M   1/2 of the day              0.5262103 0.03312537 Inf 0.46120907 0.5903352
##  F   Greater than 3/4 of the day 0.2966699 0.04761187 Inf 0.21241392 0.3974800
##  M   Greater than 3/4 of the day 0.4627165 0.05439053 Inf 0.35934466 0.5693958
##  F   Cannot judge                0.1944667 0.03628163 Inf 0.13294211 0.2754198
##  M   Cannot judge                0.3301627 0.05083482 Inf 0.23903685 0.4361182
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the logit scale </code></pre>
</div>
<p>But I don’t really want to show the ‘Cannot judge’ category, as I don’t think it contributes much to the explanation, so let’s exclude that category:</p>
<div class="code">
<pre class="r"><code>classical_mod_rg &lt;- ref_grid(classical_mod, 
                             at = list(sum_out = c(
                               &quot;None&quot;, 
                               &quot;Less than 1/4 of the day&quot;, 
                               &quot;1/2 of the day&quot;, 
                               &quot;Greater than 3/4 of the day&quot;)), 
                             type = &quot;response&quot;) 
classical_mod_em &lt;- emmeans(classical_mod_rg, ~ sex + sum_out, type = &quot;response&quot;)
classical_mod_df &lt;- data.frame(classical_mod_em)</code></pre>
</div>
<p>Now, let’s plot that (showing the 95% confidence bands):</p>
<div class="code">
<pre class="r"><code>ggplot(classical_mod_df, aes(y = prob, x = sum_out, colour = sex, group = sex)) +
  scale_color_manual(name=&quot;Sex&quot;, labels=c(&quot;Female&quot;, &quot;Male&quot;), values=c(&quot;#c90120&quot;, &quot;#0371b1&quot;)) +
  scale_fill_manual(name=&quot;Sex&quot;, labels=c(&quot;Female&quot;, &quot;Male&quot;), values=c(&quot;#c90120&quot;, &quot;#0371b1&quot;)) +
  geom_ribbon(aes(ymin = asymp.LCL, ymax = asymp.UCL, fill = sex), linetype = 0, alpha = 0.4) +
  geom_point(size = 1) + 
  geom_line() +
  xlab(&quot;Time Spent Outdoors During Summer&quot;) +
  ylab(&quot;Predicted Probability of Superhuman Vision&quot;) +
  coord_cartesian(ylim=c(0, 1)) +
  scale_y_continuous(breaks=seq(0, 1, 0.1)) +
  theme_light(base_size = 16) +
  theme(legend.position=c(0.93,0.885)) +
  theme(axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0))) +
  ggtitle(&quot;Predicted probability of superhuman vision as a function of sex and time spent 
          outdoors during summer&quot;)</code></pre>
</div>
<p><img src="/figs/pred_sumout.png" alt="png" width="800"/></p>
<p>Let’s contrast this with a penalised regression approach.</p>
</div>
<div id="machine-learning-penalised-regression" class="section level3">
<h3>Machine Learning (Penalised Regression)</h3>
</div>
