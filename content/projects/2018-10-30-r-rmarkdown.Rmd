---
title: "Predictors of SuperHuman Vision"
author: "Paul Sanfilippo"
date: 2018-10-30T21:13:14-05:00
description: "I contrasted classical statistical and machine learning approaches in identifying predictors for high-level vision in young adults."
categories: ["R"]
tags: ["regression", "plot"]
featured: true
image: "/img/super.png"

---

```{r, echo=FALSE}
library(rmarkdown)
library(knitr)
```

<style>
div.code pre { 
                font-family: 'Source Code Pro', 'Courier New', monospace;
                font-size: 11px;
                background-color:#F5F8FA;
                padding-top: 10px;
                padding-bottom: 10px;
                padding-left: 10px;
                border: 1px solid lightgrey;
                border-radius: 5px;
}
</style>

<style>
div.output pre { 
                font-family: 'Source Code Pro', 'Courier New', monospace;
                font-size: 11px;
                padding-top: 10px;
                padding-bottom: 10px;
                padding-left: 10px;
                border: 1px solid lightgrey;
                border-radius: 5px;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = F)
```

<br>

I wanted to model the relationship between various environmental, lifestyle, maternal, childhood and ophthalmic factors and best-corrected [visual acuity](https://en.wikipedia.org/wiki/Visual_acuity) (BCVA) in a young adult cohort. Data were available for 1252 patients and included 47 potential predictor variables.  

A secondary aim was to compare the predictive power of a classical modelling method (logistic regression) with a machine learning technique (penalised regression). An excellent overview of the differences can be found in this [Nature](https://www.nature.com/articles/nmeth.4642) paper.  

SuperHuman vision was defined as BCVA better than or equal to 6/4.8 vision (-0.1 logMAR) in at least one eye.  
*Note* - ‘Normal’ vision is considered equivalent to 0.00 logMAR (or 6/6 (20/20) on the Snellen scale more commonly utilised in clinical practice).  


### Descriptives

First, we'll load the **R** packages required for this project.

<div class = "code">
```{r packages, eval=F, tidy=F}
library(readxl) # read excel files
library(dplyr) # get data into shape
library(ggplot2) # plots
library(emmeans) # post-hoc model contrasts
library(missForest) # random forest imputation of missing data
library(glmnet) # penalised regression
library(officer) # table output for Word
library(flextable) # table output for Word
```
</div>

BCVA is currently continuous - we need to dichotomise it into 'Normal' - coded as 0, and 'SuperHuman' - coded as 1, vision categories.

<div class = "code">
```{r bcva_cat, eval=F, tidy=F}
dat$bcva_cat <- ifelse(dat$bcva_r <= -0.1 | dat$bcva_l <= -0.1, 1, 0)
```
</div>

Let's do some basic plotting. The frequency histogram for Left and Right eye data, constructed with ggplot2:

<div class = "code">
```{r hist, eval=F, tidy=F}
hist_va <-ggplot(dat, aes(x = bcva, fill = bcva < -0.1)) +
  geom_histogram(color = "black",  binwidth = 0.05, center = 0.025) +
  scale_x_continuous(limits = c(-0.4, 0.4), breaks = seq(-0.4, 0.4, 0.1)) +
  theme(legend.position = "none") +
  xlab("Best Corrected Visual Acuity (logMAR)") +
  ylab("Frequency ") +
  scale_fill_manual(values = c("white", "red")) +
  facet_grid(. ~ eye)
```
</div>


Then overlaying the classification thresholds for SuperHuman vision:

<div class = "code">
```{r hist2, eval=F, tidy=F}
hist_va + 
  geom_vline(data = dat, aes(xintercept = -0.097, color = "red"), linetype = "dashed") +
  geom_text(aes(-0.25, 420, label = "Superhuman Vision", color = "red"))
```
</div>

And we end up with:

<img src="/figs/svfig1.png" alt="png" width="800"/>

About **42%** of patients had SuperHuman vision under this definition.  

### Classical Modelling (Logistic Regression)

As the dependent (response or outcome) variable in this case is dichotomous (present/absent, yes/no, SuperHuman vision/Normal vision, 1/0), we will use [logistic regression](https://onlinecourses.science.psu.edu/stat504/node/149/) to find the best fitting (yet biologically plausible) model to describe the relationship between vision and the set of independent (predictor or explanatory) variables. This model predicts a logit (log-odds) transformation of the probability of a patient having SuperHuman vision.  

Potential predictor variables were tested individually and then in a multivariable model, and were initially selected based on biological plausibility of association (purposeful selection). The final best-fitting model is based on 1014 patients with complete data: 

<div class = "code">
```{r log_mod, eval=F, tidy=T}
summary(classical_mod <- glm(bcva_cat ~ age + sex + sph + cyl + sum_out + head_circum_age0, data = dat, family = binomial))
```
</div>

and the output:

<div class = "output">
```{r log_mod_out, eval=F, tidy=F}
## Call:
## glm(formula = bcva_cat ~ age + sex + sph + cyl + sum_out + head_circum_age0, 
##     family = binomial, data = dat)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5666  -1.0165  -0.6196   1.1182   2.5297  
## 
## Coefficients:
##                                    Estimate Std. Error z value Pr(>|z|)    
## (Intercept)                         0.02381    3.53640   0.007  0.99463    
## age                                -0.19074    0.15896  -1.200  0.23018    
## sexM                                0.71379    0.14010   5.095 3.49e-07 ***
## sph                                 0.14556    0.05496   2.649  0.00808 ** 
## cyl                                 1.74377    0.25573   6.819 9.18e-12 ***
## sum_outLess than 1/4 of the day     1.29889    0.57195   2.271  0.02315 *  
## sum_out1/2 of the day               1.36243    0.57260   2.379  0.01734 *  
## sum_outGreater than 3/4 of the day  1.10808    0.59977   1.848  0.06467 .  
## sum_outCannot judge                 0.55005    0.60266   0.913  0.36140    
## head_circum_age0                    0.07685    0.03884   1.979  0.04787 *  
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1377.7  on 1013  degrees of freedom
## Residual deviance: 1247.6  on 1004  degrees of freedom
##   (238 observations deleted due to missingness)
## AIC: 1267.6
## 
## Number of Fisher Scoring iterations: 5
```
</div>

As always, raw output is not very presentable, so let's try and do some formatting using the **flextable** package. First, manually assemble the dataframe after generating the 95% confidence intervals (C.I.'s).

<div class = "code">
```{r log_mod_format, eval=F, tidy=F}
cc <- coef(summary(classical_mod))
cc <- cc[,-3]
ci <- confint(classical_mod)
citab <- cbind(as.data.frame(cc), as.data.frame(ci))

rownames(citab) <- c(
  "Intercept", 
  "Age", 
  "Sex", 
  "Refraction - Sphere", 
  "Refraction - Cylinder", 
  "Time spent outdoors - less than 1/4 day", 
  "Time spent outdoors - 1/2 day", 
  "Time spent outdoors - more than 3/4 day", 
  "Time spent outdoors - cannot judge", 
  "Head circumference at birth")

citab_classical_mod <- cbind(
  "Variable" = rownames(citab), 
  citab[,1:2], 
  "Estimate Lower C.I." = citab[,4], 
  "Estimate Upper C.I." = citab[,5], 
  "Odds Ratio" = exp(citab[,1]), 
  "Odds Ratio lower C.I." = exp(citab[,4]), 
  "Odds Ratio upper C.I." = exp(citab[,5]), 
  "P-value" = citab[,3])
```
</div>

Then create the table as a Word document.

<div class = "code">
```{r log_mod_table, eval=F, tidy=F}
table <- regulartable(citab_classical_mod) %>%
  set_formatter_type(fmt_double = "%0.3f")
theme_vanilla(table)

doc <- read_docx() %>%
  body_add_flextable(value = table)
print(doc, target = ".../table.docx") %>% invisible()
```
</div>

And here are the more nicely formatted results:

<img src="/figs/class_table.png" alt="png" width="800"/>


**So it seems that being male, having low refractive error (spherical and cylindrical), spending more time outdoors, and having a larger head circumference at birth are all associated with higher odds of having SuperHuman vision.**


Often, visualisation of the results in the form of a plot is better for understanding and interpretation of effects than simply looking at a table of numbers. Here, we can do this by calculating the estimated marginal means (predicted means) of the log-odds of SuperHuman vision for set combinations of the predictor variables. The response can be converted to a predicted probability of SuperHuman vision which is more helpful in understanding predictor effects.  

Let's do this just for the ```Time spent outdoors``` variable.  

Calculate the estimated marginal means and convert to predicted probabilities:

<div class = "code">
```{r emm1, eval=F, tidy=F}
classical_mod_em <- emmeans(classical_mod, ~ sex + sum_out, type = "response")
```
</div>

<div class = "output">
```{r emm1_out, eval=F}
## sex sum_out                          prob         SE  df  asymp.LCL asymp.UCL
##  F   None                        0.1222504 0.06094369 Inf 0.04375343 0.2977293
##  M   None                        0.2214053 0.09738331 Inf 0.08590336 0.4625012
##  F   Less than 1/4 of the day    0.3379598 0.02877234 Inf 0.28405372 0.3964314
##  M   Less than 1/4 of the day    0.5103474 0.03199022 Inf 0.44781308 0.5725595
##  F   1/2 of the day              0.3523200 0.03007300 Inf 0.29584587 0.4132477
##  M   1/2 of the day              0.5262103 0.03312537 Inf 0.46120907 0.5903352
##  F   Greater than 3/4 of the day 0.2966699 0.04761187 Inf 0.21241392 0.3974800
##  M   Greater than 3/4 of the day 0.4627165 0.05439053 Inf 0.35934466 0.5693958
##  F   Cannot judge                0.1944667 0.03628163 Inf 0.13294211 0.2754198
##  M   Cannot judge                0.3301627 0.05083482 Inf 0.23903685 0.4361182
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the logit scale 
```
</div>

But I don't really want to show the 'Cannot judge' category, as I don't think it contributes much to the explanation, so let's exclude that category:

<div class = "code">
```{r emm2, eval=F, tidy=F}
classical_mod_rg <- ref_grid(classical_mod, 
  at = list(sum_out = c("None", "Less than 1/4 of the day", "1/2 of the day", 
                        "Greater than 3/4 of the day")), type = "response") 
classical_mod_em <- emmeans(classical_mod_rg, ~ sex + sum_out, type = "response")
classical_mod_df <- data.frame(classical_mod_em)
```
</div>

Now, let's plot that (showing the 95% confidence bands):

<div class = "code">
```{r pred_plot1, eval=F, tidy=F}
ggplot(classical_mod_df, aes(y = prob, x = sum_out, colour = sex, group = sex)) +
  scale_color_manual(name="Sex", labels=c("Female", "Male"), values=c("#c90120", "#0371b1")) +
  scale_fill_manual(name="Sex", labels=c("Female", "Male"), values=c("#c90120", "#0371b1")) +
  geom_ribbon(aes(ymin = asymp.LCL, ymax = asymp.UCL, fill = sex), linetype = 0, alpha = 0.4) +
  geom_point(size = 1) + 
  geom_line() +
  xlab("Time Spent Outdoors During Summer") +
  ylab("Predicted Probability of Superhuman Vision") +
  coord_cartesian(ylim=c(0, 1)) +
  scale_y_continuous(breaks=seq(0, 1, 0.1)) +
  theme_light(base_size = 16) +
  theme(legend.position=c(0.93,0.885)) +
  theme(axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0))) +
  ggtitle("Predicted probability of superhuman vision as a function of sex and time spent 
          outdoors during summer")
```
</div>

<img src="/figs/pred_sumout.png" alt="png" width="800"/>

Let's contrast this with a penalised regression approach.


### Machine Learning (Penalised Regression)

Penalised estimation methods operate in a similar way to ordinary least squares regression by minimising the residual sum squares (RSS). However, a penalty (or constraint) is placed on the size of the regression coefficients causing them to shrink toward zero. Therefore, penalised regression methods are also known as shrinkage or regularisation methods. If the shrinkage becomes sufficiently large, some regression coefficients are set to zero exactly - so penalised regression methods enable simultaneous coefficient estimation and variable selection. The two most familiar techniques for shrinking the regression coefficients towards zero are [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) and the [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)). In practice, the main distinction between ridge and LASSO is that the former will retain all predictors in the model, whereas the latter will force some estimates to be exactly zero, concomitantly performing variable selection.


**So, how does the LASSO compare to the classical model in terms of predictive power and the set of predictor variables selected in the model?**



Well, in order to implement the LASSO for this project, additional steps were required to prepare the dataset for analysis. The main statistical challenge to arise was how to deal with the issue of missing data. While purposeful selection in classical regression requires manual input based in part on domain expertise, the penalised methods simply demand 'equal opportunity' access to all available predictors in the dataset. Although the majority of variables did not have significant missing data (> 20%), given the large number of potential predictors the pattern of missing data was such that [listwise deletion](https://en.wikipedia.org/wiki/Listwise_deletion) would have removed almost all the available information. For this reason, missing values are imputed using an iterative imputation method based on a random forest algorithm.

Perform imputation of missing data using the **missForest** package:

<div class = "code">
```{r imp, eval=F, tidy=F}
set.seed(1234)
mldat_est <- missForest(dat, verbose = TRUE)
mldat_imp <- mldat_est$ximp
```
</div>

An estimate of the imputation performance (error) is provided as an 'out-of-bag' (OOB) error estimate of random forest. For continuous variables, imputation error is calculated as the normalised root mean square error (NRMSE), and for categorical variables, as an estimate of the proportion of falsely classified (PFC) events. Here, 9 iterations are required, producing OOB error estimates of 0.095 and 28.5%.

<div class = "output">
```{r imp_out, eval=F, tidy=F}
##      NRMSE        PFC 
## 0.09488661 0.28453554 
```
</div>

The next data preparation step involves the conversion of all categorical predictors to numeric variables as the **glmnet** package for penalised regression requires a design matrix as input. This may be done by using the *model.matrix* function in base R to expand all factors in the dataset to K-1 dummy variables (assuming a given factor had k levels).

<div class = "code">
```{r mod_mat, eval=F, tidy=T}
mod_mat <- model.matrix.lm( ~ ethnicity + sex + edu_lev + yr_comp + ftstudy + tv_hrs + 
                              egames_hrs + comp_work_hrs + comp_games_hrs + comp_social_hrs + 
                              comp_surf_hrs + sum_out + mob_phone + no_texts + freq_alc, 
                            data = mldat_imp)
mod_mat <- data.frame(mod_mat)
```
</div>

Now for the fun part. We'll randomly split the dataset into 'training' (75%) and 'test' (25%) sets using the *createDataPartition* function of the **caret** package. Creating a train/test split of the dataset provides an effective method for evaluating the performance of a predictive model. The training set is used to prepare the model and inform its decision process, while the test dataset applies that information in making predictions against the unseen test data. The performance of the model is determined by the accuracy with which the predictions match the unseen outcome values.

<div class = "code">
```{r dat_split, eval=F, tidy=F}
set.seed(1234)
training.samples <- mldat_imp1$bcva_cat %>%
  createDataPartition(p = 0.75, list = FALSE)
train.data  <- mldat_imp1[training.samples, ]
test.data <- mldat_imp1[-training.samples, ]
```
</div>

The **glmnet** algorithms use a convergence method based on cyclical coordinate descent, successively optimising the objective function over each parameter with others fixed, until the minimum of the function is found. The *cv.glmnet* function is used to perform 10(K)-fold cross-validation (CV) for optimising and selecting the tuning parameter λ, and simultaneously fit the model to the training set data. In this context, the CV procedure split the training data into 10 (K) groups of equal size, with the model being fit to 9 (K – 1) groups and the tuning parameter prediction error estimated from the 'left-out' group. The process is repeated 10 (K) times selecting the value for the tuning parameter with the smallest prediction error (λ min).

First, identify the predictor (x) and outcome (y) variables of the training data:

<div class = "code">
```{r dat_split2, eval=F, tidy=F}
x <- model.matrix(factor(bcva_cat) ~ ., train.data)[,-1]
y <- factor(train.data$bcva_cat)
```
</div>

As the outcome variable was binary (Normal vision = 0, SuperHuman vision = 1), we fit the LASSO using a logit link function as per the standard GLM, with coefficient estimates returned on the log-odds scale. The alpha value is set to 1 for LASSO (vs 0 for ridge).

<div class = "code">
```{r cv, eval=F, tidy=F}
cv <- cv.glmnet(x, y, alpha = 1, family = "binomial", type.measure = "class")
lambda.min <- cv$lambda.min
```
</div>

And the optimised tuning parameter (λ min):

<div class = "code">
```{r cv2, eval=F, tidy=F}
lambda.min
```
</div>

<div class = "output">
```{r cv_out, eval=F, tidy=F}
## [1] 0.1631945
```
</div>

Now, let's evaluate the model informed by the training data on the test data to make predictions of vision status. We'll do this with the *predict.cv.glmnet* function. First, identify the predictor (x2) and outcome (y2) variables of the test data:

<div class = "code">
```{r dat_split3, eval=F, tidy=F}
x2 <- model.matrix(factor(bcva_cat)~., test.data)[,-1]
y2 <- factor(test.data$bcva_cat)
```
</div>

Predict:

<div class = "code">
```{r dat_split4, eval=F, tidy=F}
pred <- predict.cv.glmnet(cv, newx = x2, s = "lambda.min", type = "class")
pred <- data.frame(pred)
```
</div>

But how do we know how well the model predicts vision status from this? Well, to assess model performance a 'confusion matrix' is generated with the *confusionMatrix* function of the **caret** package. A confusion matrix summarises the classification performance of the trained model on the test data. As a two-dimensional matrix, one dimension is indexed by the predicted class of the object and the other by the true class. 

<img src="/figs/conf_mat.png" alt="png" width="800"/>

Then, a broad measure of classification performance may be defined as accuracy, whereby:

<img src="/figs/conf_mat2.png" alt="png" width="400"/>

Let's calculate the confusion matrix:

<div class = "code">
```{r conf_mat, eval=F, tidy=F}
confusionMatrix(pred$X1, y2, positive = "1")
```
</div>

<div class = "output">
```{r conf_mat_out, eval=F, tidy=F}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 172  81
##          1  19  41
##                                           
##                Accuracy : 0.681          
##                  95% CI : (0.626, 0.732)
##     No Information Rate : 0.6102          
##     P-Value [Acc > NIR] : 0.006       
##                                           
##                   Kappa : 0.261          
##  Mcnemar's Test P-Value : 0.000       
##                                           
##             Sensitivity : 0.336          
##             Specificity : 0.901          
##          Pos Pred Value : 0.683          
##          Neg Pred Value : 0.680         
##              Prevalence : 0.390          
##          Detection Rate : 0.131          
##    Detection Prevalence : 0.192          
##       Balanced Accuracy : 0.618          
##                                           
##        'Positive' Class : 1  
```
</div>

The accuracy of LASSO in correctly predicting vision status is about **68%**.

Let's repeat this same process, using the *same* train/test split of the data, with the classical model.

<div class = "code">
```{r glm_pred, eval=F, tidy=T}
set.seed(1234)
training.samples2 <- dat$bcva_cat %>%
  createDataPartition(p = 0.75, list = FALSE)
train.data2  <- data.frame(dat[training.samples2, ])
test.data2 <- data.frame(dat[-training.samples2, ])

glm.train <- glm(bcva_cat ~ age + sex + sph + cyl + sum_out + head_circum_age0, data = train.data2, family = binomial)
glm.pred <- predict(glm.train, newdata = test.data2, type='response')
glm.class <- factor(ifelse(glm.pred > 0.5, 1, 0))
y3 <- factor(test.data2$bcva_cat)

confusionMatrix(glm.class, y3, positive = "1")
```
</div>

<div class = "output">
```{r conf_mat_out2, eval=F, tidy=F}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 111  41
##          1  42  57
##                                           
##                Accuracy : 0.669          
##                  95% CI : (0.607, 0.727)
##     No Information Rate : 0.610          
##     P-Value [Acc > NIR] : 0.029         
##                                           
##                   Kappa : 0.307          
##  Mcnemar's Test P-Value : 1.000         
##                                           
##             Sensitivity : 0.582          
##             Specificity : 0.725          
##          Pos Pred Value : 0.575          
##          Neg Pred Value : 0.730          
##              Prevalence : 0.390          
##          Detection Rate : 0.227          
##    Detection Prevalence : 0.394          
##       Balanced Accuracy : 0.653          
##                                           
##        'Positive' Class : 1   
```
</div>

The accuracy of the classical model in correctly predicting vision status is about **67%** - really not much difference in predictive performance.

Let's have a look at those two models respective confusion matrices, just to understand a little better what the numbers mean. For each model summary the frequency coloured <span style="color:red">**red**</span> represents the number of individuals in the test set data correctly identified as having SuperHuman vision (TP), and the frequency coloured <span style="color:blue">**blue**</span> the number of individuals correctly identified as having Normal vision (TN). <span style="color:green">**Green**</span> cells indicate the number of individuals with SuperHuman vision incorrectly classified by the model as having Normal vision (FN). Similarly, <span style="color:orange">**orange**</span> cells give the number of individuals with Normal vision that the model incorrectly classified as having SuperHuman vision (FP).

<img src="/figs/conf_mat3.png" alt="png" width="800"/>

The final part of this project is to look at what predictors the LASSO selected. 

<div class = "code">
```{r feat, eval=F, tidy=F}
myCoefs <- coef(cv, s = "lambda.min");
myCoefs[which(myCoefs != 0 ) ] 
myCoefs@Dimnames[[1]][which(myCoefs != 0 ) ]
myResults <- data.frame(
  features = myCoefs@Dimnames[[1]][ which(myCoefs != 0 ) ],
  coefs    = myCoefs              [ which(myCoefs != 0 ) ]
)
myResults
```
</div>

We can see that most of these appear in the classical model, so it appears that both approaches have produced models based on a similar set of predictor variables.

<div class = "output">
```{r feat_out, eval=F, tidy=F}
##                         features         coefs
## 1                    (Intercept) -6.033976e+00
## 2                          sph_r  7.697420e-02
## 3                          cyl_r  5.272135e-01
## 4                          cyl_l -2.655846e-01
## 5                         rnfl_r  1.028928e-02
## 6              head_circum_birth  9.225977e-03
## 7                           sexM  8.760413e-02
## 8          sum_out1.2.of.the.day  1.930521e-01
```
</div>

The identification of significant predictors for the classical regression component of this work was based on domain knowledge and purposeful selection techniques. While penalised regression methods do not explicitly choose certain variables, by virtue of the penalty applied to the coefficient estimates the algorithm passively automates predictor selection by shrinking some coefficients to zero. It was therefore of interest to contrast the commonality of the variables in the final models for the two approaches, and indeed, almost the same set of predictors were shared by both methods. The other salient finding was that the classical model had comparable predictive power to the penalised techniques. The advantage of the former being that it's grounded in a mathematical model that facilitates inference and greater overall interpretability.

The juxtaposition of classical statistical and machine learning approaches in this work was intended as an expository exercise. However, it does highlight that while not mutually exclusive in the capacity to derive similar data insights, the methods also share different objectives. For example, [Bzdok et. al.](https://www.nature.com/articles/nmeth.4642) maintain a distinction in which inference versus prediction is the fundamental goal – statistical learning for drawing population inferences from a sample, and machine learning for finding generalisable predictive patterns. In a landmark 2001 paper, [Breiman](http://www2.math.uu.se/~thulin/mm/breiman.pdf) advocated for the adoption of a more diverse set of tools to augment conventional data modelling. Certainly, advancements in computing technology have facilitated the recent, rapid uptake and application of machine learning methods in many areas of health science. Going forward it seems inevitable that both classical statistical and machine learning will act in complement to provide biologically meaningful conclusions in the process of scientific discovery.



